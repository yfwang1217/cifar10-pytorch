{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import numpy as np\n",
        "\n",
        "# Function to load CIFAR-10 dataset from binary pickle files\n",
        "def unpickle(file):\n",
        "    import pickle\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='bytes')\n",
        "    return dict\n",
        "\n",
        "# Define paths to the data files\n",
        "file_paths = [\n",
        "    'data_batch_1',\n",
        "    'data_batch_2',\n",
        "    'data_batch_3',\n",
        "    'data_batch_4',\n",
        "    'data_batch_5'\n",
        "]\n",
        "\n",
        "unlabeled_file_path = 'cifar_test_nolabels.pkl'\n",
        "\n",
        "# Detect and set the appropriate device (GPU or CPU)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "# Load and prepare the training data\n",
        "batch_data = []\n",
        "batch_label = []\n",
        "for file_path in file_paths:\n",
        "    batch = unpickle(file_path)\n",
        "    batch_data.append(batch[b'data'])\n",
        "    batch_label.append(batch[b'labels'])\n",
        "train_data = np.concatenate(batch_data)\n",
        "train_labels = np.concatenate(batch_label)\n",
        "print(train_data.shape, train_labels.shape)\n",
        "\n",
        "# Load and prepare the test data\n",
        "test_data_dict = unpickle('test_batch')\n",
        "test_data = np.array(test_data_dict[b'data'])\n",
        "test_labels = np.array(test_data_dict[b'labels'])\n",
        "\n",
        "# Load and prepare unlabeled data for semi-supervised learning\n",
        "unlabeled_data_dict = unpickle(unlabeled_file_path)\n",
        "unlabeled_data = np.array(unlabeled_data_dict[b'data'])\n",
        "\n",
        "# Define the dataset class for CIFAR-10\n",
        "class CIFAR10Dataset(Dataset):\n",
        "    def __init__(self, data, labels=None, transform=None):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image = self.data[index].reshape(3, 32, 32).transpose(1, 2, 0)\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        if self.labels is not None:\n",
        "            label = self.labels[index]\n",
        "            return image, label\n",
        "        return image, -1  # Return -1 for unlabeled data\n",
        "\n",
        "# Data transformations for training and testing\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "])\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "])\n",
        "\n",
        "# Create dataset objects\n",
        "train_dataset = CIFAR10Dataset(train_data, train_labels, transform=transform_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "# Data loaders for handling batches\n",
        "test_dataset = CIFAR10Dataset(test_data, test_labels, transform=transform_test)\n",
        "test_loader = DataLoader(test_dataset, batch_size=100, shuffle=False)\n",
        "\n",
        "unlabeled_dataset = CIFAR10Dataset(unlabeled_data, transform=transform_test)\n",
        "unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=100, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6az_jcl1_Lo8",
        "outputId": "bcba8d41-f476-46a0-a01d-7e16e79aafb9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n",
            "(50000, 3072) (50000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
        "                               planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "# Define the ResNet architecture\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 32  # Reduced from 64 to adjust to CIFAR-10\n",
        "        # Additional layers and network initialization\n",
        "        # Implement the network forward pass\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.layer1 = self._make_layer(block, 32, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 64, num_blocks[1], stride=2)\n",
        "\n",
        "        self.layer3 = self._make_layer(block, 128, max(1, num_blocks[2]-1), stride=2)  # 减少一个block\n",
        "        self.linear = nn.Linear(512, num_classes)\n",
        "\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "          layers.append(block(self.in_planes, planes, stride))\n",
        "          self.in_planes = planes * block.expansion\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "def resnet34():\n",
        "  return ResNet(BasicBlock,[3,4,6,3])\n",
        "# Instantiate and train the network\n",
        "model = resnet34().to(device)"
      ],
      "metadata": {
        "id": "boCK9QED_v0F"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, data_loader, optimizer, criterion, device, lr_scheduler=None):\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if lr_scheduler is not None:\n",
        "            lr_scheduler.step()\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        epoch_acc += torch.sum(preds == targets).item()\n",
        "        epoch_loss += loss.item()\n",
        "    average_loss = epoch_loss / len(data_loader)\n",
        "    average_accuracy = epoch_acc / (len(data_loader) * data_loader.batch_size)\n",
        "\n",
        "    return average_loss, average_accuracy"
      ],
      "metadata": {
        "id": "TW2PKDImW9cO"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, data_loader, criterion, device):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            if len(batch) == 2:\n",
        "                inputs, targets = batch[0].to(device), batch[1].to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, targets)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                epoch_acc += torch.sum(preds == targets).item()\n",
        "                epoch_loss += loss.item()\n",
        "                total_samples += targets.size(0)\n",
        "            elif len(batch) == 1:  # Handling data loaders that only return inputs (for predictions)\n",
        "                inputs = batch[0].to(device)\n",
        "                outputs = model(inputs)\n",
        "                # Optionally handle outputs if needed (for example logging or further processing)\n",
        "\n",
        "    if total_samples > 0:\n",
        "        average_loss = epoch_loss / len(data_loader)\n",
        "        average_accuracy = epoch_acc / total_samples\n",
        "    else:\n",
        "        average_loss = None  # No loss computed if there are no targets\n",
        "        average_accuracy = None  # No accuracy computed if there are no targets\n",
        "\n",
        "    return average_loss, average_accuracy\n"
      ],
      "metadata": {
        "id": "XFby1JwcW-yi"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "from torchsummary import summary\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.utils.data as data\n",
        "import matplotlib.pyplot as plt\n",
        "import urllib.request\n",
        "from PIL import Image\n",
        "import json\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "import numpy as np\n",
        "from torchvision.transforms import autoaugment\n",
        "EPOCHS=35\n",
        "model = resnet34().to(device)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "steps_per_epoch = len(train_loader)\n",
        "total_steps = steps_per_epoch * EPOCHS\n",
        "lr_scheduler = OneCycleLR(optimizer, max_lr=0.003, steps_per_epoch=steps_per_epoch, epochs=EPOCHS, div_factor=10, pct_start=0.3)\n",
        "\n",
        "\n",
        "train_losses = []\n",
        "train_accuracies = []\n",
        "val_losses = []\n",
        "val_accuracies = []\n",
        "\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    try:\n",
        "        train_loss, train_acc = train(model, train_loader, optimizer, criterion, device, lr_scheduler)\n",
        "    except Exception as e:\n",
        "        print(f\"Training failed during epoch {epoch + 1}: {e}\")\n",
        "        train_loss, train_acc = None, None\n",
        "\n",
        "    try:\n",
        "        val_loss, val_acc = evaluate(model, test_loader, criterion, device)\n",
        "    except Exception as e:\n",
        "        print(f\"Evaluation failed during epoch {epoch + 1}: {e}\")\n",
        "        val_loss, val_acc = None, None\n",
        "\n",
        "    train_loss_str = f\"{train_loss:.3f}\" if train_loss is not None else \"N/A\"\n",
        "    train_acc_str = f\"{train_acc:.2f}%\" if train_acc is not None else \"N/A\"\n",
        "    val_loss_str = f\"{val_loss:.3f}\" if val_loss is not None else \"N/A\"\n",
        "    val_acc_str = f\"{val_acc:.2f}%\" if val_acc is not None else \"N/A\"\n",
        "\n",
        "    print(f'Epoch: {epoch + 1}, Training Loss: {train_loss_str}, Training Accuracy: {train_acc_str}, Validation Loss: {val_loss_str}, Validation Accuracy: {val_acc_str}')\n",
        "\n",
        "    if val_acc is not None and val_acc == max(val_accuracies, default=float('-inf')):\n",
        "        torch.save(model.state_dict(), 'best_model3.pth')\n",
        "        print(\"Model saved at epoch:\", epoch + 1)"
      ],
      "metadata": {
        "id": "OU1F3SosXAPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "model.load_state_dict(torch.load('best_model3.pth'))\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
        "print(f\"The final test accuracy is: {test_acc*100:.2f}% and the test loss is: {test_loss:.3f}\")\n",
        "\n",
        "with open('cifar_test_nolabels.pkl', 'rb') as file:\n",
        "    test_data_dict = pickle.load(file)\n",
        "\n",
        "class CIFARTestDataset(Dataset):\n",
        "    def __init__(self, data_dict, transform=None):\n",
        "        self.data = data_dict[b'data']\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.data[idx].reshape(3, 32, 32).transpose(1, 2, 0)\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image\n",
        "\n",
        "# Initialize Dataset and DataLoader\n",
        "test_dataset = CIFARTestDataset(data_dict=test_data_dict, transform=transform_test)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
        "\n",
        "def predict(model, test_loader):\n",
        "    predictions = []\n",
        "    with torch.no_grad():\n",
        "        for inputs in test_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            predictions.extend(predicted.cpu().numpy())\n",
        "    return predictions\n",
        "\n",
        "# Get predictions\n",
        "predictions = predict(model, test_loader)\n",
        "\n",
        "# Prepare and save the CSV file\n",
        "ids = list(range(10000))\n",
        "df = pd.DataFrame({'ID': ids, 'Labels': predictions})\n",
        "df.to_csv('submission_3.csv', index=False)\n"
      ],
      "metadata": {
        "id": "CIok4XmQXMoS"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s0Zy1LNPYLLr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}